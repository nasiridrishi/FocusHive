#!/usr/bin/env python3

import os
import json
import xml.etree.ElementTree as ET
from pathlib import Path
from collections import defaultdict
import re

def parse_junit_xml(xml_file):
    """Parse a single JUnit XML file and extract test results"""
    try:
        tree = ET.parse(xml_file)
        root = tree.getroot()
        
        # Handle different XML structures
        if root.tag == 'testsuite':
            testsuites = [root]
        elif root.tag == 'testsuites':
            testsuites = root.findall('testsuite')
        else:
            print(f"Warning: Unknown XML root tag '{root.tag}' in {xml_file}")
            return None
        
        results = []
        
        for testsuite in testsuites:
            suite_name = testsuite.get('name', 'Unknown')
            suite_tests = int(testsuite.get('tests', 0))
            suite_failures = int(testsuite.get('failures', 0))
            suite_errors = int(testsuite.get('errors', 0))
            suite_skipped = int(testsuite.get('skipped', 0))
            suite_time = float(testsuite.get('time', 0.0))
            
            # Process individual test cases
            for testcase in testsuite.findall('testcase'):
                test_name = testcase.get('name', 'Unknown')\n                test_classname = testcase.get('classname', 'Unknown')\n                test_time = float(testcase.get('time', 0.0))\n                \n                # Check for failures and errors\n                failure = testcase.find('failure')\n                error = testcase.find('error')\n                skipped = testcase.find('skipped')\n                \n                status = 'passed'\n                failure_message = None\n                failure_type = None\n                failure_stacktrace = None\n                \n                if failure is not None:\n                    status = 'failed'\n                    failure_message = failure.get('message', 'No message')\n                    failure_type = failure.get('type', 'Unknown')\n                    failure_stacktrace = failure.text or 'No stacktrace'\n                elif error is not None:\n                    status = 'error'\n                    failure_message = error.get('message', 'No message')\n                    failure_type = error.get('type', 'Unknown')\n                    failure_stacktrace = error.text or 'No stacktrace'\n                elif skipped is not None:\n                    status = 'skipped'\n                    failure_message = skipped.get('message', 'Skipped')\n                \n                test_result = {\n                    'test_name': test_name,\n                    'class_name': test_classname,\n                    'full_name': f\"{test_classname}.{test_name}\",\n                    'status': status,\n                    'duration_seconds': test_time,\n                    'failure_message': failure_message,\n                    'failure_type': failure_type,\n                    'failure_stacktrace': failure_stacktrace,\n                    'suite_name': suite_name\n                }\n                \n                results.append(test_result)\n        \n        return results\n        \n    except ET.ParseError as e:\n        print(f\"Error parsing XML file {xml_file}: {e}\")\n        return None\n    except Exception as e:\n        print(f\"Unexpected error parsing {xml_file}: {e}\")\n        return None\n\ndef extract_root_cause(failure_message, failure_stacktrace):\n    \"\"\"Extract a concise root cause from failure message and stacktrace\"\"\"\n    if not failure_message and not failure_stacktrace:\n        return \"Unknown error\"\n    \n    # Use failure message if available\n    if failure_message:\n        # Clean up common patterns\n        message = failure_message.strip()\n        # Truncate very long messages\n        if len(message) > 150:\n            message = message[:150] + \"...\"\n        return message\n    \n    # Extract from stacktrace\n    if failure_stacktrace:\n        lines = failure_stacktrace.strip().split('\\n')\n        # Look for the first meaningful line\n        for line in lines:\n            line = line.strip()\n            if line and not line.startswith('at ') and not line.startswith('...'):\n                if len(line) > 150:\n                    line = line[:150] + \"...\"\n                return line\n    \n    return \"Error details unavailable\"\n\ndef analyze_batch_results(batch_name, xml_results_dir):\n    \"\"\"Analyze test results for a single batch\"\"\"\n    xml_dir = Path(xml_results_dir)\n    if not xml_dir.exists():\n        return None\n    \n    batch_results = {\n        'batch_name': batch_name,\n        'total_tests': 0,\n        'passed_tests': 0,\n        'failed_tests': 0,\n        'error_tests': 0,\n        'skipped_tests': 0,\n        'total_duration_seconds': 0,\n        'test_results': [],\n        'failure_summary': [],\n        'error_patterns': defaultdict(int)\n    }\n    \n    # Find all XML files\n    xml_files = list(xml_dir.glob('**/TEST-*.xml'))\n    \n    if not xml_files:\n        print(f\"No XML files found in {xml_dir}\")\n        return batch_results\n    \n    # Parse each XML file\n    for xml_file in xml_files:\n        results = parse_junit_xml(xml_file)\n        if results:\n            batch_results['test_results'].extend(results)\n    \n    # Analyze results\n    for test in batch_results['test_results']:\n        batch_results['total_tests'] += 1\n        batch_results['total_duration_seconds'] += test['duration_seconds']\n        \n        if test['status'] == 'passed':\n            batch_results['passed_tests'] += 1\n        elif test['status'] == 'failed':\n            batch_results['failed_tests'] += 1\n            \n            # Extract root cause\n            root_cause = extract_root_cause(test['failure_message'], test['failure_stacktrace'])\n            batch_results['failure_summary'].append({\n                'test_name': test['full_name'],\n                'root_cause': root_cause,\n                'failure_type': test['failure_type']\n            })\n            \n            # Track error patterns\n            if test['failure_type']:\n                batch_results['error_patterns'][test['failure_type']] += 1\n            \n        elif test['status'] == 'error':\n            batch_results['error_tests'] += 1\n            \n            # Extract root cause\n            root_cause = extract_root_cause(test['failure_message'], test['failure_stacktrace'])\n            batch_results['failure_summary'].append({\n                'test_name': test['full_name'],\n                'root_cause': root_cause,\n                'failure_type': test['failure_type']\n            })\n            \n            # Track error patterns\n            if test['failure_type']:\n                batch_results['error_patterns'][test['failure_type']] += 1\n                \n        elif test['status'] == 'skipped':\n            batch_results['skipped_tests'] += 1\n    \n    return batch_results\n\ndef main():\n    \"\"\"Main function to parse all batch results\"\"\"\n    artifacts_dir = Path('artifacts')\n    \n    if not artifacts_dir.exists():\n        print(\"Artifacts directory not found. Please run test batches first.\")\n        return\n    \n    # Find all batch directories\n    batch_dirs = [d for d in artifacts_dir.iterdir() if d.is_dir() and d.name.startswith('batch-')]\n    \n    if not batch_dirs:\n        print(\"No batch result directories found in artifacts/\")\n        return\n    \n    print(f\"Found {len(batch_dirs)} batch result directories\")\n    \n    all_results = {\n        'analysis_timestamp': '2025-09-19T21:55:07',  # Will be updated\n        'total_batches': 0,\n        'total_tests': 0,\n        'total_passed': 0,\n        'total_failed': 0,\n        'total_errors': 0,\n        'total_skipped': 0,\n        'batch_results': [],\n        'overall_error_patterns': defaultdict(int),\n        'top_failure_causes': []\n    }\n    \n    # Process each batch\n    for batch_dir in sorted(batch_dirs):\n        batch_name = batch_dir.name.replace('batch-', '')\n        xml_results_dir = batch_dir / 'xml-results'\n        \n        print(f\"Processing batch: {batch_name}\")\n        \n        batch_results = analyze_batch_results(batch_name, xml_results_dir)\n        \n        if batch_results:\n            all_results['batch_results'].append(batch_results)\n            all_results['total_batches'] += 1\n            all_results['total_tests'] += batch_results['total_tests']\n            all_results['total_passed'] += batch_results['passed_tests']\n            all_results['total_failed'] += batch_results['failed_tests']\n            all_results['total_errors'] += batch_results['error_tests']\n            all_results['total_skipped'] += batch_results['skipped_tests']\n            \n            # Aggregate error patterns\n            for error_type, count in batch_results['error_patterns'].items():\n                all_results['overall_error_patterns'][error_type] += count\n    \n    # Convert defaultdict to regular dict for JSON serialization\n    all_results['overall_error_patterns'] = dict(all_results['overall_error_patterns'])\n    \n    # Create top failure causes\n    failure_causes = defaultdict(int)\n    for batch in all_results['batch_results']:\n        for failure in batch['failure_summary']:\n            cause = failure['root_cause'][:100]  # Truncate for grouping\n            failure_causes[cause] += 1\n    \n    all_results['top_failure_causes'] = sorted(\n        [{'cause': cause, 'count': count} for cause, count in failure_causes.items()],\n        key=lambda x: x['count'],\n        reverse=True\n    )[:10]\n    \n    # Save results\n    os.makedirs('results', exist_ok=True)\n    with open('results/parsed_test_results.json', 'w') as f:\n        json.dump(all_results, f, indent=2, default=str)\n    \n    # Print summary\n    print(f\"\\n{'='*80}\")\n    print(\"TEST RESULTS ANALYSIS SUMMARY\")\n    print(f\"{'='*80}\")\n    print(f\"Total batches processed: {all_results['total_batches']}\")\n    print(f\"Total tests: {all_results['total_tests']}\")\n    print(f\"Passed: {all_results['total_passed']}\")\n    print(f\"Failed: {all_results['total_failed']}\")\n    print(f\"Errors: {all_results['total_errors']}\")\n    print(f\"Skipped: {all_results['total_skipped']}\")\n    \n    if all_results['total_tests'] > 0:\n        pass_rate = (all_results['total_passed'] / all_results['total_tests']) * 100\n        print(f\"Pass rate: {pass_rate:.1f}%\")\n    \n    print(f\"\\nTop Error Types:\")\n    for error_type, count in sorted(all_results['overall_error_patterns'].items(), key=lambda x: x[1], reverse=True)[:5]:\n        print(f\"  {error_type}: {count}\")\n    \n    print(f\"\\nTop Failure Causes:\")\n    for cause in all_results['top_failure_causes'][:5]:\n        print(f\"  {cause['count']}x: {cause['cause']}\")\n    \n    print(f\"\\nDetailed results saved to: results/parsed_test_results.json\")\n\nif __name__ == \"__main__\":\n    main()